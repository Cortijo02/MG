{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh7vMAD5F1pF"
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Práctica 1: Autoencoders y VAEs\n",
    "\n",
    "<i><small>Authors: Félix  Fuentes Hurtado<br>Last update: 2023-10-11</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdz-_-v3bI1v"
   },
   "source": [
    "En esta práctica vais a poner en juego los conocimientos adquiridos sobre Autoencoders y Autoencoders Variacionales. Para ello, llevaréis a cabo las siguientes tareas:\n",
    "\n",
    "- Construir y entrenar un VAE **convolucional**.\n",
    "- Re-implementar el VAE convolucional empleando el GradientTape de TensorFlow y un *training loop* personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjXJmobjcahd"
   },
   "source": [
    "## Parte 1: Implementación de un VAE convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iGG7D3xR7zj"
   },
   "source": [
    "Antes de nada, obtendremos y prepararemos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_BKWfDieR7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZouz_ODds_x"
   },
   "source": [
    "Ahora definiremos el encoder y el decoder, así como la función que permitirá entrenar al VAE haciendo uso del \"reparameterization trick\".\n",
    "\n",
    "Primero crearemos una capa personalizada para realizar el [\"truco de reparametrización\"](https://www.baeldung.com/cs/vae-reparameterization).\n",
    "\n",
    "**Pregunta**: Lee el enlace anterior y explica brevemente en qué consiste el \"truco de reparametrización\".\n",
    "\n",
    "\n",
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KThPnrzXcfrl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "input_dim = (28, 28, 1)\n",
    "latent_dim = 2\n",
    "\n",
    "# encoder\n",
    "## Aquí tu código ##\n",
    "input_img = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "x = layers.Conv2D(16, (3,3), activation = \"relu\", padding = \"same\")(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3,3), activation = \"relu\", padding = \"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(4, (3,3), activation = \"relu\", padding = \"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_sigma = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling (args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "# Create encoder\n",
    "encoder = keras.Model(input_img, [z_mean, z_log_sigma, z], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bGLdWxI8QEar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(encoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m__yqSyYckS-"
   },
   "outputs": [],
   "source": [
    "# Create decoder\n",
    "\n",
    "## Aquí tu código ##\n",
    "\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPsh076OQZb9"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(decoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pYV_MgOQa6z"
   },
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "vae = keras.Model(## Aquí tu código ##, name='vae_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGjHD3rfP4ka"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(vae, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ9F_c9ncktM"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "reconstruction_loss = ## Aquí tu código ##\n",
    "## Aquí tu código ##\n",
    "\n",
    "kl_loss = ## Aquí tu código ##\n",
    "## Aquí tu código ##\n",
    "\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUzpBhstPlIX"
   },
   "outputs": [],
   "source": [
    "vae.fit(## Aquí tu código ##,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CFaZLrPlHRL"
   },
   "source": [
    "Y con esto, tenemos nuestro primer VAE Convolucional implementado.\n",
    "\n",
    "Veamos las muestras que podemos generar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkNUq7bQcqC-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(decoder, n=15, figsize=15):\n",
    "    # display an n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(decoder, n=20, figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djYwpjZLX3vg"
   },
   "source": [
    "Y el espacio latente, tanto del conjunto de entrenamiento como del de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwfN9nBwEyG9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train_encoded = encoder.predict(x_train, batch_size=32)\n",
    "x_train_encoded = np.asarray(x_train_encoded)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[2, :, 0], x_train_encoded[2, :, 1], c=y_train)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6fWCJ6rpnIx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=32)\n",
    "x_test_encoded = np.asarray(x_test_encoded)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[2, :, 0], x_test_encoded[2, :, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOGG_v1RE6Ty"
   },
   "source": [
    "**Pregunta**: ¿Qué diferencias observas?\n",
    "\n",
    "**Respuesta**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_gfWbUrlaE1"
   },
   "source": [
    "## Parte 2: Re-implementar el VAE convolucional empleando el GradientTape de TensorFlow y un *training loop* personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho9qs2sLllUe"
   },
   "source": [
    "La forma de programar con `GradientTape` y la creación de un bucle de entrenamiento personalizado en Keras ofrece ventajas significativas en comparación con la API secuencial y funcional de Keras en situaciones específicas. Algunas de estas ventajas son:\n",
    "\n",
    "1. **Flexibilidad completa**: Con un bucle de entrenamiento personalizado tenéis un control total sobre cada paso del proceso de entrenamiento. Podéis implementar algoritmos de entrenamiento altamente personalizados y manejar situaciones que no son fáciles de abordar con modelos secuenciales o funcionales predefinidos, como por ejemplo, estructuras complejas.\n",
    "\n",
    "2. **Investigación y experimentación**: Para investigadores y científicos que necesitan probar hipótesis, realizar experimentos y explorar nuevas arquitecturas de modelos, la programación personalizada con `GradientTape` brinda la flexibilidad necesaria. Podéis modificar el comportamiento del modelo y el proceso de entrenamiento en cada iteración.\n",
    "\n",
    "3. **Regulación y pérdidas personalizadas**: Algunas aplicaciones requieren funciones de pérdida personalizadas o métodos de regularización no disponibles en la API secuencial o funcional. Con un bucle de entrenamiento personalizado, podéis implementar y ajustar estas funciones según sea necesario.\n",
    "\n",
    "4. **Control absoluto de los gradientes**: `GradientTape` os permite acceder y modificar los gradientes en cada paso del entrenamiento, lo que puede ser esencial para aplicar técnicas de optimización avanzadas, como gradientes acoplados, cálculo de gradientes en funciones no diferenciables o personalización de reglas de actualización de pesos, por ejemplo.\n",
    "\n",
    "5. **Análisis e inspección detallada**: Al escribir un bucle de entrenamiento personalizado, podéis realizar un seguimiento y análisis detallado de métricas e información en cada paso del entrenamiento, lo que es útil para el depuración, la monitorización y la recopilación de datos para la investigación.\n",
    "\n",
    "Sin embargo, es importante destacar que las API secuenciales y funcionales de Keras son ideales para tareas comunes y suelen ser más fáciles de usar y más eficientes en términos de código.\n",
    "\n",
    "La programación personalizada es más adecuada cuando se requiere un mayor control y adaptabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEZ1CE7ysF8-"
   },
   "source": [
    "Para llevar a cabo esta implementación, deberéis completar los siguientes pasos:\n",
    "\n",
    "- Definición de la arquitectura propia haciendo uso de `tf.keras.Model`\n",
    "- Definición de la función de pérdidas\n",
    "- Definición del comportamiento del modelo durante el *forward pass* (método `train_step` de la clase)\n",
    "- Compilar `model.compile(...)` y entrenar `model.fit(...)` el modelo.\n",
    "\n",
    "Estos son los pasos básicos.\n",
    "\n",
    "Sin embargo, veremos que de esta forma todavía es `Keras` quien se encarga de \"alimentar\" el modelo. Esto es un paso intermedio entre la máxima personalización y las APIs funcionales y secuenciales que ofrece Keras. Todavía es posible personalizar más el proceso de entrenamiento, pero eso ya se lo dejo a quien le interese :)\n",
    "\n",
    "Podéis leer sobre ello aquí: https://keras.io/guides/writing_a_training_loop_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIb3E6AOmVQz"
   },
   "source": [
    "### Definición de la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPa1CrgbxjP7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sae_mnCAGglX"
   },
   "source": [
    "Primero crearemos una capa personalizada implementando el [\"truco de reparametrización\"](https://www.baeldung.com/cs/vae-reparameterization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAQE5oz3G5gR"
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkihipYCG72F"
   },
   "source": [
    "Ahora la arquitectura en sí: el encoder, el decoder y sus comportamientos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h08Pwykalegm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ConvolutionalVAE(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = self.get_encoder(input_shape)\n",
    "        self.decoder = self.get_decoder(latent_dim)\n",
    "\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    # mediante esta porción de código definimos las métricas que queremos\n",
    "    # vigilar durante el entrenamiento\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    # define the encoder\n",
    "    def get_encoder(self, input_shape=(28, 28, 1), latent_dim=2):\n",
    "\n",
    "        # encoder\n",
    "        encoder_inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "        ## Aquí tu código ##\n",
    "\n",
    "        encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    # define the decoder\n",
    "    def get_decoder(self, latent_dim):\n",
    "\n",
    "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "        ## Aquí tu código ##\n",
    "\n",
    "        decoder = tf.keras.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    # aquí podemos definir lo que queremos que ocurra en cada iteración del\n",
    "    # bucle de entrenamiento\n",
    "    def train_step(self, data):\n",
    "\n",
    "        # el GradientTape es el objeto que llevará un control de los gradientes\n",
    "        # y el que permitirá que se actualicen los pesos\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # codificamos los datos de entrada\n",
    "            z_mean, z_log_var, z = ## Aquí tu código ##\n",
    "\n",
    "            # reconstruimos los datos de entrada\n",
    "            reconstruction = ## Aquí tu código ##\n",
    "\n",
    "            # calculamos las pérdidas de reconstrucción\n",
    "            reconstruction_loss = ## Aquí tu código ##\n",
    "\n",
    "            # calculamos las pérdidas KL\n",
    "            kl_loss = ## Aquí tu código ##\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        # obtenemos los gradientes de los parámetros (pesos y bias) a ajustar (entrenar)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "\n",
    "        # optimizamos los pesos (los ajustamos, aquí es donde realmente se entrena la red)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # actualizamos las métricas\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTZwBoz6_Um-"
   },
   "source": [
    "Ya tenemos la arquitectura definida, junto con las pérdidas y el `train_step`.\n",
    "\n",
    "Ya solo nos queda alimentar el modelo y entrenarlo, y comprobar los resultados.\n",
    "\n",
    "Vamos a preparar los datos del MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFWGuLFIIbpQ"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tt_dPxdCCnQ"
   },
   "source": [
    "Ahora instanciamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_QbBQNXDPAj"
   },
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "latent_dims = 2\n",
    "vae = ConvolutionalVAE(input_shape, latent_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC8KgxF8DmVm"
   },
   "source": [
    "Lo compilamos y por último, lo entrenamos.\n",
    "\n",
    "Fijaos en que en esta ocasión, como tenemos control sobre el *training loop*, le introducimos solo los datos `x_train`, ya que en el `train_step` solo usaremos esos datos (como ambas entradas a la función de pérdidas de reconstrucción)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fst8cj_rDmqR"
   },
   "outputs": [],
   "source": [
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "vae.fit(x_train, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMa1W5nXJ-VR"
   },
   "source": [
    "Vamos a visualizar un grid de imágenes generadas con nuestro VAE para ver qué tal funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKTdDqSeJ63v"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=15, figsize=15):\n",
    "    # display an n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(vae, n=20, figsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_VMTd55J9nG"
   },
   "source": [
    "Y ahora lo mismo pero con los códigos latentes, tanto de train como de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ha56Y4bzb2Z"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train_encoded = vae.encoder.predict(x_train, batch_size=32)\n",
    "x_train_encoded = np.asarray(x_train_encoded)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[0, :, 0], x_train_encoded[0, :, 1], c=y_train)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asJEK11D0HOj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_test_encoded = vae.encoder.predict(x_test, batch_size=32)\n",
    "x_test_encoded = np.asarray(x_test_encoded)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[0, :, 0], x_test_encoded[0, :, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLWJ4_ybF3KE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "263px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
